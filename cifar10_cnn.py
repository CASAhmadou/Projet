"""
Projet de Recherche: Classification d'images CIFAR-10 avec CNN
================================================================
Auteur: Projet Deep Learning
Date: Octobre 2025
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os

# Configuration pour la reproductibilitÃ©
np.random.seed(42)
tf.random.set_seed(42)

print(f"TensorFlow version: {tf.__version__}")
print(f"GPU disponible: {tf.config.list_physical_devices('GPU')}")

# ==============================================================================
# 1. CHARGEMENT ET PRÃ‰TRAITEMENT DES DONNÃ‰ES
# ==============================================================================

print("\n" + "="*80)
print("1. CHARGEMENT DES DONNÃ‰ES CIFAR-10")
print("="*80)

# Charger CIFAR-10
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

# Noms des classes
class_names = ['avion', 'auto', 'oiseau', 'chat', 'cerf', 
               'chien', 'grenouille', 'cheval', 'bateau', 'camion']

print(f"\nDimensions du jeu d'entraÃ®nement: {x_train.shape}")
print(f"Dimensions du jeu de test: {x_test.shape}")
print(f"Nombre de classes: {len(class_names)}")

# Normalisation des images [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Conversion des labels en one-hot encoding
y_train_cat = keras.utils.to_categorical(y_train, 10)
y_test_cat = keras.utils.to_categorical(y_test, 10)

# Split validation
from sklearn.model_selection import train_test_split
x_train, x_val, y_train_cat, y_val_cat = train_test_split(
    x_train, y_train_cat, test_size=0.1, random_state=42
)

print(f"\nAprÃ¨s split validation:")
print(f"Train: {x_train.shape[0]} images")
print(f"Validation: {x_val.shape[0]} images")
print(f"Test: {x_test.shape[0]} images")

# Visualisation d'exemples
def plot_sample_images(x, y, class_names, n=10):
    """Affiche un Ã©chantillon d'images"""
    plt.figure(figsize=(15, 3))
    for i in range(n):
        plt.subplot(2, 5, i + 1)
        plt.imshow(x[i])
        plt.title(class_names[np.argmax(y[i])])
        plt.axis('off')
    plt.tight_layout()
    plt.savefig('echantillon_images.png', dpi=150, bbox_inches='tight')
    print("\nâœ“ Ã‰chantillon d'images sauvegardÃ©: echantillon_images.png")
    plt.show()

plot_sample_images(x_train, y_train_cat, class_names)

# ==============================================================================
# 2. CONSTRUCTION DU MODÃˆLE CNN
# ==============================================================================

print("\n" + "="*80)
print("2. CONSTRUCTION DU MODÃˆLE CNN")
print("="*80)

def create_base_model():
    """
    ModÃ¨le CNN de base (conforme aux consignes minimales)
    Architecture: 2xConv2D + 2xMaxPooling + Flatten + Dense
    """
    model = models.Sequential([
        # Premier bloc convolutionnel
        layers.Conv2D(32, (3, 3), activation='relu', padding='same', 
                      input_shape=(32, 32, 3), name='conv1'),
        layers.MaxPooling2D((2, 2), name='pool1'),
        
        # DeuxiÃ¨me bloc convolutionnel
        layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2'),
        layers.MaxPooling2D((2, 2), name='pool2'),
        
        # Aplatissement et classification
        layers.Flatten(name='flatten'),
        layers.Dense(128, activation='relu', name='dense1'),
        layers.Dense(10, activation='softmax', name='output')
    ], name='CNN_Base')
    
    return model

def create_improved_model():
    """
    ModÃ¨le CNN amÃ©liorÃ© avec BatchNorm et Dropout (BONUS)
    Architecture plus profonde avec rÃ©gularisation
    """
    model = models.Sequential([
        # Bloc 1
        layers.Conv2D(32, (3, 3), activation='relu', padding='same', 
                      input_shape=(32, 32, 3)),
        layers.BatchNormalization(),
        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.2),
        
        # Bloc 2
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.3),
        
        # Bloc 3
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.4),
        
        # Classification
        layers.Flatten(),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(10, activation='softmax')
    ], name='CNN_Improved')
    
    return model

# CrÃ©er les deux modÃ¨les
model_base = create_base_model()
model_improved = create_improved_model()

print("\n--- ModÃ¨le de base ---")
model_base.summary()

print("\n--- ModÃ¨le amÃ©liorÃ© ---")
model_improved.summary()

# Choisir le modÃ¨le Ã  entraÃ®ner (modÃ¨le amÃ©liorÃ© par dÃ©faut)
model = model_improved

# ==============================================================================
# 3. COMPILATION ET HYPERPARAMÃˆTRES
# ==============================================================================

print("\n" + "="*80)
print("3. COMPILATION DU MODÃˆLE")
print("="*80)

# HyperparamÃ¨tres
LEARNING_RATE = 0.001
BATCH_SIZE = 64
EPOCHS = 50

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print(f"\nHyperparamÃ¨tres:")
print(f"  - Learning rate: {LEARNING_RATE}")
print(f"  - Batch size: {BATCH_SIZE}")
print(f"  - Epochs max: {EPOCHS}")
print(f"  - Optimizer: Adam")
print(f"  - Loss: categorical_crossentropy")

# ==============================================================================
# 4. DATA AUGMENTATION (BONUS)
# ==============================================================================

print("\n" + "="*80)
print("4. DATA AUGMENTATION (BONUS)")
print("="*80)

datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    zoom_range=0.1
)

datagen.fit(x_train)
print("âœ“ Data augmentation configurÃ©e")

# ==============================================================================
# 5. CALLBACKS (BONUS)
# ==============================================================================

print("\n" + "="*80)
print("5. CONFIGURATION DES CALLBACKS")
print("="*80)

# CrÃ©er dossier pour sauvegarder le modÃ¨le
os.makedirs('models', exist_ok=True)

callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    ),
    ModelCheckpoint(
        'models/best_model.keras',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-6,
        verbose=1
    )
]

print("âœ“ Callbacks configurÃ©s: EarlyStopping, ModelCheckpoint, ReduceLROnPlateau")

# ==============================================================================
# 6. ENTRAÃNEMENT
# ==============================================================================

print("\n" + "="*80)
print("6. ENTRAÃNEMENT DU MODÃˆLE")
print("="*80)

history = model.fit(
    datagen.flow(x_train, y_train_cat, batch_size=BATCH_SIZE),
    epochs=EPOCHS,
    validation_data=(x_val, y_val_cat),
    callbacks=callbacks,
    verbose=1
)

print("\nâœ“ EntraÃ®nement terminÃ©!")

# ==============================================================================
# 7. VISUALISATION DES COURBES D'APPRENTISSAGE
# ==============================================================================

print("\n" + "="*80)
print("7. COURBES D'APPRENTISSAGE")
print("="*80)

def plot_training_history(history):
    """Tracer les courbes de loss et accuracy"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Loss
    axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)
    axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)
    axes[0].set_title('Ã‰volution de la Loss', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Accuracy
    axes[1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)
    axes[1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)
    axes[1].set_title('Ã‰volution de l\'Accuracy', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('courbes_apprentissage.png', dpi=150, bbox_inches='tight')
    print("âœ“ Courbes sauvegardÃ©es: courbes_apprentissage.png")
    plt.show()

plot_training_history(history)

# ==============================================================================
# 8. Ã‰VALUATION SUR LE JEU DE TEST
# ==============================================================================

print("\n" + "="*80)
print("8. Ã‰VALUATION SUR LE JEU DE TEST")
print("="*80)

# PrÃ©dictions
y_pred_prob = model.predict(x_test, verbose=0)
y_pred = np.argmax(y_pred_prob, axis=1)
y_true = y_test.flatten()

# MÃ©triques globales
test_loss, test_acc = model.evaluate(x_test, y_test_cat, verbose=0)
print(f"\nRÃ©sultats sur le jeu de test:")
print(f"  - Test Loss: {test_loss:.4f}")
print(f"  - Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)")

# Rapport de classification dÃ©taillÃ©
print("\n--- Rapport de classification ---")
print(classification_report(y_true, y_pred, target_names=class_names))

# ==============================================================================
# 9. MATRICE DE CONFUSION
# ==============================================================================

print("\n" + "="*80)
print("9. MATRICE DE CONFUSION")
print("="*80)

def plot_confusion_matrix(y_true, y_pred, class_names):
    """Tracer la matrice de confusion"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Nombre de prÃ©dictions'})
    plt.title('Matrice de Confusion', fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('PrÃ©diction', fontsize=12)
    plt.ylabel('VÃ©ritÃ©', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig('matrice_confusion.png', dpi=150, bbox_inches='tight')
    print("âœ“ Matrice de confusion sauvegardÃ©e: matrice_confusion.png")
    plt.show()
    
    # Calculer les accuracies par classe
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    print("\nAccuracy par classe:")
    for i, name in enumerate(class_names):
        print(f"  {name:12s}: {cm_normalized[i, i]:.2%}")

plot_confusion_matrix(y_true, y_pred, class_names)

# ==============================================================================
# 10. EXEMPLES DE PRÃ‰DICTIONS
# ==============================================================================

print("\n" + "="*80)
print("10. EXEMPLES DE PRÃ‰DICTIONS")
print("="*80)

def plot_predictions(x, y_true, y_pred, y_prob, class_names, correct=True, n=10):
    """Afficher des exemples de prÃ©dictions correctes ou incorrectes"""
    if correct:
        indices = np.where(y_true == y_pred)[0]
        title = "PrÃ©dictions CORRECTES"
        filename = "predictions_correctes.png"
    else:
        indices = np.where(y_true != y_pred)[0]
        title = "PrÃ©dictions INCORRECTES"
        filename = "predictions_incorrectes.png"
    
    # SÃ©lectionner n exemples alÃ©atoires
    selected_indices = np.random.choice(indices, min(n, len(indices)), replace=False)
    
    plt.figure(figsize=(15, 6))
    for i, idx in enumerate(selected_indices):
        plt.subplot(2, 5, i + 1)
        plt.imshow(x[idx])
        
        true_label = class_names[y_true[idx]]
        pred_label = class_names[y_pred[idx]]
        confidence = y_prob[idx, y_pred[idx]] * 100
        
        if correct:
            color = 'green'
            label = f"âœ“ {pred_label}\n{confidence:.1f}%"
        else:
            color = 'red'
            label = f"âœ— PrÃ©dit: {pred_label}\nVrai: {true_label}\n{confidence:.1f}%"
        
        plt.title(label, color=color, fontsize=9, fontweight='bold')
        plt.axis('off')
    
    plt.suptitle(title, fontsize=14, fontweight='bold', y=1.02)
    plt.tight_layout()
    plt.savefig(filename, dpi=150, bbox_inches='tight')
    print(f"âœ“ {title} sauvegardÃ©es: {filename}")
    plt.show()

# Afficher exemples corrects
plot_predictions(x_test, y_true, y_pred, y_pred_prob, class_names, correct=True)

# Afficher exemples incorrects
plot_predictions(x_test, y_true, y_pred, y_pred_prob, class_names, correct=False)

# ==============================================================================
# 11. ANALYSE DES ERREURS
# ==============================================================================

print("\n" + "="*80)
print("11. ANALYSE DES ERREURS")
print("="*80)

# Trouver les classes les plus confondues
cm = confusion_matrix(y_true, y_pred)
np.fill_diagonal(cm, 0)  # Ignorer les prÃ©dictions correctes

most_confused = []
for i in range(10):
    for j in range(10):
        if cm[i, j] > 0:
            most_confused.append((class_names[i], class_names[j], cm[i, j]))

most_confused.sort(key=lambda x: x[2], reverse=True)

print("\nTop 10 des confusions les plus frÃ©quentes:")
for i, (true_class, pred_class, count) in enumerate(most_confused[:10], 1):
    print(f"{i:2d}. {true_class:12s} â†’ {pred_class:12s} : {count} fois")

# ==============================================================================
# 12. RÃ‰SUMÃ‰ DU PROJET
# ==============================================================================

print("\n" + "="*80)
print("12. RÃ‰SUMÃ‰ DU PROJET")
print("="*80)

print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    RÃ‰SUMÃ‰ DU PROJET CIFAR-10 CNN                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š DONNÃ‰ES:
   â€¢ Dataset: CIFAR-10 (60,000 images 32x32 RGB)
   â€¢ Train: {x_train.shape[0]} images
   â€¢ Validation: {x_val.shape[0]} images
   â€¢ Test: {x_test.shape[0]} images
   â€¢ Classes: {len(class_names)}

ğŸ—ï¸ ARCHITECTURE:
   â€¢ Type: CNN amÃ©liorÃ© avec BatchNorm et Dropout
   â€¢ Couches Conv2D: 7 couches
   â€¢ Couches MaxPooling: 3 couches
   â€¢ ParamÃ¨tres totaux: {model.count_params():,}

âš™ï¸ HYPERPARAMÃˆTRES:
   â€¢ Optimizer: Adam (lr={LEARNING_RATE})
   â€¢ Batch size: {BATCH_SIZE}
   â€¢ Epochs entraÃ®nÃ©s: {len(history.history['loss'])}
   â€¢ Data Augmentation: âœ“ ActivÃ©e

ğŸ“ˆ RÃ‰SULTATS:
   â€¢ Test Accuracy: {test_acc*100:.2f}%
   â€¢ Test Loss: {test_loss:.4f}
   
ğŸ¯ BONUS IMPLÃ‰MENTÃ‰S:
   âœ“ Data Augmentation (rotation, shift, flip, zoom)
   âœ“ Dropout et BatchNormalization
   âœ“ Comparaison d'architectures (base vs amÃ©liorÃ©e)
   âœ“ Callbacks (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau)
   âœ“ Analyse dÃ©taillÃ©e des erreurs
   
ğŸ“ FICHIERS GÃ‰NÃ‰RÃ‰S:
   â€¢ echantillon_images.png
   â€¢ courbes_apprentissage.png
   â€¢ matrice_confusion.png
   â€¢ predictions_correctes.png
   â€¢ predictions_incorrectes.png
   â€¢ models/best_model.keras

â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

print("\nâœ… Projet terminÃ© avec succÃ¨s!")
print("ğŸ“ Tous les rÃ©sultats ont Ã©tÃ© sauvegardÃ©s.")
print("ğŸ“„ Utilisez ces Ã©lÃ©ments pour rÃ©diger votre rapport de 3 pages.")

# ==============================================================================
# NOTES POUR LE RAPPORT
# ==============================================================================

print("\n" + "="*80)
print("NOTES POUR VOTRE RAPPORT (3 PAGES)")
print("="*80)

print("""
ğŸ“„ STRUCTURE SUGGÃ‰RÃ‰E DU RAPPORT:

1. INTRODUCTION (0.5 page)
   â€¢ Contexte: Classification d'images, importance du Deep Learning
   â€¢ Objectif: Classer 10 catÃ©gories d'objets avec CIFAR-10
   â€¢ Dataset: 60,000 images 32x32 RGB

2. MÃ‰THODOLOGIE (1 page)
   â€¢ Architecture CNN choisie:
     - Justifier le choix (profondeur, BatchNorm, Dropout)
     - SchÃ©ma de l'architecture
   â€¢ HyperparamÃ¨tres:
     - Learning rate, batch size, optimizer (Adam)
     - Justifier les choix
   â€¢ Data Augmentation: techniques utilisÃ©es et pourquoi
   â€¢ Callbacks: prÃ©vention de l'overfitting

3. RÃ‰SULTATS (1 page)
   â€¢ Courbes d'apprentissage: analyse convergence et overfitting
   â€¢ MÃ©triques finales: accuracy, loss
   â€¢ Matrice de confusion: classes bien/mal prÃ©dites
   â€¢ Exemples de prÃ©dictions

4. CONCLUSION (0.5 page)
   â€¢ RÃ©sultats atteints vs objectifs
   â€¢ DifficultÃ©s rencontrÃ©es (classes confondues)
   â€¢ AmÃ©liorations possibles:
     * Architectures plus profondes (ResNet, EfficientNet)
     * Transfer Learning (modÃ¨les prÃ©-entraÃ®nÃ©s)
     * Augmentation plus agressive
     * Ensembling de modÃ¨les

FIGURES Ã€ INCLURE:
âœ“ Courbes d'apprentissage
âœ“ Matrice de confusion
âœ“ Exemples de prÃ©dictions (correctes et incorrectes)
âœ“ SchÃ©ma de l'architecture (optionnel)

POINTS CLÃ‰S Ã€ MENTIONNER:
â€¢ Normalisation des donnÃ©es [0,1]
â€¢ Split train/val/test
â€¢ Data augmentation pour rÃ©duire l'overfitting
â€¢ BatchNorm pour stabiliser l'apprentissage
â€¢ Dropout pour la rÃ©gularisation
â€¢ EarlyStopping pour Ã©viter l'overfitting
â€¢ Analyse des confusions entre classes similaires
""")

print("\n" + "="*80)
print("FIN DU SCRIPT")
print("="*80)